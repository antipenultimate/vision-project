{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import keras\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fer2013():\n",
    "    if not os.path.exists(\"fer2013\"):\n",
    "        print(\"Downloading the face emotion dataset...\")\n",
    "        subprocess.check_output(\"curl -SL https://www.dropbox.com/s/opuvvdv3uligypx/fer2013.tar | tar xz\", shell=True)\n",
    "    data = pd.read_csv(\"fer2013/fer2013.csv\")\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = np.asarray(pixel_sequence.split(' '), dtype=np.uint8).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'), (width, height))\n",
    "        faces.append(face.astype('float32'))\n",
    "\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
    "\n",
    "    val_faces = faces[int(len(faces) * 0.8):]\n",
    "    val_emotions = emotions[int(len(faces) * 0.8):]\n",
    "    train_faces = faces[:int(len(faces) * 0.8)]\n",
    "    train_emotions = emotions[:int(len(faces) * 0.8)]\n",
    "    \n",
    "    return train_faces, train_emotions, val_faces, val_emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "\n",
    "train_faces, train_emotions, val_faces, val_emotions = load_fer2013()\n",
    "num_samples, num_classes = train_emotions.shape\n",
    "\n",
    "train_faces /= 255.\n",
    "val_faces /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "datagen.fit(train_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = run.config\n",
    "\n",
    "config.batch_size = 32\n",
    "#config.num_epochs = 20\n",
    "\n",
    "input_shape = (48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added from cnn.py\n",
    "#config = run.config\n",
    "config.first_layer_convs = 32\n",
    "config.first_layer_conv_width = 3\n",
    "config.first_layer_conv_height = 3\n",
    "#config.dropout = 0.2\n",
    "config.dense_layer_size = 128\n",
    "#config.img_width = 28\n",
    "#config.img_height = 28\n",
    "config.num_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/qualcomm/emotion-aug21/runs/3m7nbfcg\n",
      "Wrap your training loop with `with wandb.monitor():` to display live results.\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28709 samples, validate on 7178 samples\n",
      "Epoch 1/32\n",
      "28709/28709 [==============================] - 17s 584us/step - loss: 1.7498 - acc: 0.2869 - val_loss: 1.6037 - val_acc: 0.3806\n",
      "Epoch 2/32\n",
      "28709/28709 [==============================] - 12s 433us/step - loss: 1.5600 - acc: 0.3901 - val_loss: 1.4386 - val_acc: 0.4487\n",
      "Epoch 3/32\n",
      "28709/28709 [==============================] - 12s 424us/step - loss: 1.4644 - acc: 0.4325 - val_loss: 1.3631 - val_acc: 0.4634\n",
      "Epoch 4/32\n",
      "28709/28709 [==============================] - 12s 424us/step - loss: 1.4056 - acc: 0.4572 - val_loss: 1.3366 - val_acc: 0.4891\n",
      "Epoch 5/32\n",
      "28709/28709 [==============================] - 12s 427us/step - loss: 1.3553 - acc: 0.4794 - val_loss: 1.2733 - val_acc: 0.5185\n",
      "Epoch 6/32\n",
      "28709/28709 [==============================] - 12s 425us/step - loss: 1.3273 - acc: 0.4912 - val_loss: 1.2439 - val_acc: 0.5231\n",
      "Epoch 7/32\n",
      "28709/28709 [==============================] - 12s 425us/step - loss: 1.3024 - acc: 0.5010 - val_loss: 1.2251 - val_acc: 0.5315\n",
      "Epoch 8/32\n",
      "28709/28709 [==============================] - 12s 427us/step - loss: 1.2750 - acc: 0.5149 - val_loss: 1.2041 - val_acc: 0.5425\n",
      "Epoch 9/32\n",
      "28709/28709 [==============================] - 12s 427us/step - loss: 1.2567 - acc: 0.5197 - val_loss: 1.1932 - val_acc: 0.5437\n",
      "Epoch 10/32\n",
      "28709/28709 [==============================] - 12s 427us/step - loss: 1.2408 - acc: 0.5281 - val_loss: 1.1827 - val_acc: 0.5546\n",
      "Epoch 11/32\n",
      "28709/28709 [==============================] - 12s 424us/step - loss: 1.2317 - acc: 0.5323 - val_loss: 1.1854 - val_acc: 0.5436\n",
      "Epoch 12/32\n",
      "28709/28709 [==============================] - 12s 426us/step - loss: 1.2112 - acc: 0.5383 - val_loss: 1.1621 - val_acc: 0.5546\n",
      "Epoch 13/32\n",
      "28709/28709 [==============================] - 12s 425us/step - loss: 1.1976 - acc: 0.5436 - val_loss: 1.1837 - val_acc: 0.5497\n",
      "Epoch 14/32\n",
      "28709/28709 [==============================] - 12s 419us/step - loss: 1.1918 - acc: 0.5479 - val_loss: 1.1568 - val_acc: 0.5593\n",
      "Epoch 15/32\n",
      "28709/28709 [==============================] - 12s 426us/step - loss: 1.1794 - acc: 0.5525 - val_loss: 1.1482 - val_acc: 0.5626\n",
      "Epoch 16/32\n",
      "28709/28709 [==============================] - 12s 427us/step - loss: 1.1655 - acc: 0.5563 - val_loss: 1.1397 - val_acc: 0.5717\n",
      "Epoch 17/32\n",
      "28709/28709 [==============================] - 12s 425us/step - loss: 1.1577 - acc: 0.5619 - val_loss: 1.1379 - val_acc: 0.5652\n",
      "Epoch 18/32\n",
      "28709/28709 [==============================] - 12s 428us/step - loss: 1.1397 - acc: 0.5675 - val_loss: 1.1437 - val_acc: 0.5644\n",
      "Epoch 19/32\n",
      "28709/28709 [==============================] - 12s 429us/step - loss: 1.1375 - acc: 0.5673 - val_loss: 1.1239 - val_acc: 0.5713\n",
      "Epoch 20/32\n",
      "28709/28709 [==============================] - 12s 425us/step - loss: 1.1237 - acc: 0.5748 - val_loss: 1.1265 - val_acc: 0.5737\n",
      "Epoch 21/32\n",
      "28709/28709 [==============================] - 12s 428us/step - loss: 1.1190 - acc: 0.5721 - val_loss: 1.1373 - val_acc: 0.5720\n",
      "Epoch 22/32\n",
      "28709/28709 [==============================] - 12s 428us/step - loss: 1.1180 - acc: 0.5759 - val_loss: 1.1222 - val_acc: 0.5770\n",
      "Epoch 23/32\n",
      "28709/28709 [==============================] - 12s 427us/step - loss: 1.1018 - acc: 0.5814 - val_loss: 1.1240 - val_acc: 0.5766\n",
      "Epoch 24/32\n",
      "28709/28709 [==============================] - 12s 426us/step - loss: 1.0923 - acc: 0.5826 - val_loss: 1.1044 - val_acc: 0.5834\n",
      "Epoch 25/32\n",
      "28709/28709 [==============================] - 12s 426us/step - loss: 1.0867 - acc: 0.5860 - val_loss: 1.1107 - val_acc: 0.5797\n",
      "Epoch 26/32\n",
      "28709/28709 [==============================] - 13s 441us/step - loss: 1.0776 - acc: 0.5951 - val_loss: 1.1142 - val_acc: 0.5761\n",
      "Epoch 27/32\n",
      "28709/28709 [==============================] - 13s 453us/step - loss: 1.0731 - acc: 0.5940 - val_loss: 1.1115 - val_acc: 0.5844\n",
      "Epoch 28/32\n",
      "28709/28709 [==============================] - 12s 430us/step - loss: 1.0728 - acc: 0.5968 - val_loss: 1.1209 - val_acc: 0.5776\n",
      "Epoch 29/32\n",
      "28709/28709 [==============================] - 13s 441us/step - loss: 1.0600 - acc: 0.6014 - val_loss: 1.1147 - val_acc: 0.5747\n",
      "Epoch 30/32\n",
      "28709/28709 [==============================] - 12s 415us/step - loss: 1.0528 - acc: 0.6023 - val_loss: 1.1163 - val_acc: 0.5822\n",
      "Epoch 31/32\n",
      "28709/28709 [==============================] - 12s 411us/step - loss: 1.0567 - acc: 0.5985 - val_loss: 1.1004 - val_acc: 0.5846\n",
      "Epoch 32/32\n",
      "28709/28709 [==============================] - 12s 415us/step - loss: 1.0400 - acc: 0.6043 - val_loss: 1.0953 - val_acc: 0.5894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0ed52c400>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "#config.num_epochs = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), \n",
    "    #(config.first_layer_conv_width, config.first_layer_conv_height),\n",
    "    input_shape=(48, 48,1),\n",
    "    padding='valid',\n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), \n",
    "    padding='valid',\n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), \n",
    "    padding='valid',\n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64*7, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#model.load_weights(\"emotion.h5\")\n",
    "\n",
    "model.fit(train_faces, train_emotions, batch_size=config.batch_size,\n",
    "        epochs=config.num_epochs, verbose=1, callbacks=[\n",
    "            WandbCallback(data_type=\"image\", labels=[\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"])\n",
    "        ], validation_data=(val_faces, val_emotions))\n",
    "\n",
    "\n",
    "#train_generator = datagen.flow(train_faces, train_emotions, batch_size=32)\n",
    "#val_generator = datagen.flow(val_faces, val_emotions, batch_size=32)\n",
    "#model.fit_generator(train_generator,\n",
    "#            steps_per_epoch=len(train_faces)//32, \n",
    "#            epochs=config.num_epochs, verbose=1,\n",
    "#            #callbacks=[\n",
    "#            #    WandbCallback(data_type=\"image\", labels=[\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"])\n",
    "#            #],\n",
    "#            validation_data=val_generator,\n",
    "#            validation_steps=len(val_faces)//32)\n",
    "\n",
    "\n",
    "#for e in range(config.num_epochs):\n",
    "#    print(\"Epoch\", e)\n",
    "#    batches = 0\n",
    "#    for (x_batch, y_batch), (val_x, val_y) in zip(\n",
    "#                    datagen.flow(train_faces, train_emotions, batch_size=32),\n",
    "#                    datagen.flow(val_faces, val_emotions, batch_size=32)):\n",
    "#        model.fit(x_batch, y_batch, validation_data=(val_x, val_y), verbose=0)\n",
    "#        batches += 1\n",
    "#        if batches >= len(train_faces) / 32:\n",
    "#            break\n",
    "#    model.evaluate(val_faces, val_emotions)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_faces)//32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"emotion.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
