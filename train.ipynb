{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import keras\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fer2013():\n",
    "    if not os.path.exists(\"fer2013\"):\n",
    "        print(\"Downloading the face emotion dataset...\")\n",
    "        subprocess.check_output(\"curl -SL https://www.dropbox.com/s/opuvvdv3uligypx/fer2013.tar | tar xz\", shell=True)\n",
    "    data = pd.read_csv(\"fer2013/fer2013.csv\")\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = np.asarray(pixel_sequence.split(' '), dtype=np.uint8).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'), (width, height))\n",
    "        faces.append(face.astype('float32'))\n",
    "\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
    "\n",
    "    val_faces = faces[int(len(faces) * 0.8):]\n",
    "    val_emotions = emotions[int(len(faces) * 0.8):]\n",
    "    train_faces = faces[:int(len(faces) * 0.8)]\n",
    "    train_emotions = emotions[:int(len(faces) * 0.8)]\n",
    "    \n",
    "    return train_faces, train_emotions, val_faces, val_emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "\n",
    "train_faces, train_emotions, val_faces, val_emotions = load_fer2013()\n",
    "num_samples, num_classes = train_emotions.shape\n",
    "\n",
    "train_faces /= 255.\n",
    "val_faces /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        #rotation_range=20,\n",
    "        #width_shift_range=0.2,\n",
    "        #height_shift_range=0.2,\n",
    "        #rescale=1./255,\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "datagen.fit(train_faces, augment=True, rounds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B Run: https://app.wandb.ai/qualcomm/emotion-aug21/runs/y0osny07\n",
      "Wrap your training loop with `with wandb.monitor():` to display live results.\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init()\n",
    "input_shape = (48, 48, 1)\n",
    "config = run.config\n",
    "\n",
    "#config.first_layer_convs = 32\n",
    "#config.first_layer_conv_width = 3\n",
    "#config.first_layer_conv_height = 3\n",
    "#config.dropout = 0.2\n",
    "#config.dense_layer_size = 128\n",
    "#config.img_width = 28\n",
    "#config.img_height = 28\n",
    "config.num_epochs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "897/897 [==============================] - 15s 17ms/step - loss: 1.7172 - acc: 0.3053 - val_loss: 1.5335 - val_acc: 0.4140\n",
      "Epoch 2/32\n",
      "897/897 [==============================] - 14s 16ms/step - loss: 1.5206 - acc: 0.4109 - val_loss: 1.3938 - val_acc: 0.4643\n",
      "Epoch 3/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.4330 - acc: 0.4444 - val_loss: 1.3566 - val_acc: 0.4819\n",
      "Epoch 4/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.3761 - acc: 0.4708 - val_loss: 1.2757 - val_acc: 0.5116\n",
      "Epoch 5/32\n",
      "897/897 [==============================] - 13s 15ms/step - loss: 1.3434 - acc: 0.4834 - val_loss: 1.2542 - val_acc: 0.5223\n",
      "Epoch 6/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.3184 - acc: 0.4939 - val_loss: 1.2230 - val_acc: 0.5281\n",
      "Epoch 7/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.2903 - acc: 0.5070 - val_loss: 1.2433 - val_acc: 0.5203\n",
      "Epoch 8/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.2709 - acc: 0.5163 - val_loss: 1.2061 - val_acc: 0.5365\n",
      "Epoch 9/32\n",
      "897/897 [==============================] - 13s 15ms/step - loss: 1.2567 - acc: 0.5221 - val_loss: 1.1794 - val_acc: 0.5493\n",
      "Epoch 10/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.2418 - acc: 0.5268 - val_loss: 1.2029 - val_acc: 0.5411\n",
      "Epoch 11/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.2283 - acc: 0.5344 - val_loss: 1.1992 - val_acc: 0.5496\n",
      "Epoch 12/32\n",
      "897/897 [==============================] - 14s 16ms/step - loss: 1.2122 - acc: 0.5360 - val_loss: 1.1631 - val_acc: 0.5554\n",
      "Epoch 13/32\n",
      "897/897 [==============================] - 15s 17ms/step - loss: 1.1975 - acc: 0.5440 - val_loss: 1.1745 - val_acc: 0.5525\n",
      "Epoch 14/32\n",
      "897/897 [==============================] - 16s 18ms/step - loss: 1.1922 - acc: 0.5470 - val_loss: 1.1536 - val_acc: 0.5624\n",
      "Epoch 15/32\n",
      "897/897 [==============================] - 15s 16ms/step - loss: 1.1793 - acc: 0.5530 - val_loss: 1.1450 - val_acc: 0.5646\n",
      "Epoch 16/32\n",
      "897/897 [==============================] - 14s 16ms/step - loss: 1.1694 - acc: 0.5588 - val_loss: 1.1542 - val_acc: 0.5624\n",
      "Epoch 17/32\n",
      "897/897 [==============================] - 14s 16ms/step - loss: 1.1630 - acc: 0.5592 - val_loss: 1.1453 - val_acc: 0.5648\n",
      "Epoch 18/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.1499 - acc: 0.5648 - val_loss: 1.1433 - val_acc: 0.5692\n",
      "Epoch 19/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.1391 - acc: 0.5692 - val_loss: 1.1178 - val_acc: 0.5776\n",
      "Epoch 20/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.1332 - acc: 0.5713 - val_loss: 1.1370 - val_acc: 0.5702\n",
      "Epoch 21/32\n",
      "897/897 [==============================] - 14s 15ms/step - loss: 1.1226 - acc: 0.5735 - val_loss: 1.1167 - val_acc: 0.5789\n",
      "Epoch 22/32\n",
      "897/897 [==============================] - 15s 16ms/step - loss: 1.1133 - acc: 0.5798 - val_loss: 1.1195 - val_acc: 0.5789\n",
      "Epoch 23/32\n",
      "897/897 [==============================] - 15s 16ms/step - loss: 1.1128 - acc: 0.5806 - val_loss: 1.1357 - val_acc: 0.5772\n",
      "Epoch 24/32\n",
      "897/897 [==============================] - 15s 17ms/step - loss: 1.1018 - acc: 0.5867 - val_loss: 1.1142 - val_acc: 0.5766\n",
      "Epoch 25/32\n",
      "897/897 [==============================] - 16s 18ms/step - loss: 1.0980 - acc: 0.5834 - val_loss: 1.1115 - val_acc: 0.5783\n",
      "Epoch 26/32\n",
      "897/897 [==============================] - 16s 17ms/step - loss: 1.0772 - acc: 0.5918 - val_loss: 1.1243 - val_acc: 0.5804\n",
      "Epoch 27/32\n",
      "897/897 [==============================] - 16s 18ms/step - loss: 1.0755 - acc: 0.5929 - val_loss: 1.1015 - val_acc: 0.5942\n",
      "Epoch 28/32\n",
      "897/897 [==============================] - 16s 18ms/step - loss: 1.0594 - acc: 0.5981 - val_loss: 1.0941 - val_acc: 0.5971\n",
      "Epoch 29/32\n",
      "897/897 [==============================] - 16s 18ms/step - loss: 1.0651 - acc: 0.5989 - val_loss: 1.1133 - val_acc: 0.5901\n",
      "Epoch 30/32\n",
      "897/897 [==============================] - 15s 17ms/step - loss: 1.0477 - acc: 0.6084 - val_loss: 1.0838 - val_acc: 0.6010\n",
      "Epoch 31/32\n",
      "897/897 [==============================] - 16s 17ms/step - loss: 1.0487 - acc: 0.6034 - val_loss: 1.1074 - val_acc: 0.5892\n",
      "Epoch 32/32\n",
      "897/897 [==============================] - 16s 17ms/step - loss: 1.0429 - acc: 0.6075 - val_loss: 1.1038 - val_acc: 0.5929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f638529d4a8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), \n",
    "    #(config.first_layer_conv_width, config.first_layer_conv_height),\n",
    "    input_shape=(48, 48,1),\n",
    "    padding='valid',\n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), \n",
    "    padding='valid',\n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), \n",
    "    padding='valid',\n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8*64*7, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=1*0.001) #, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#model.load_weights(\"emotion.h5\")\n",
    "\n",
    "#model.fit(train_faces, train_emotions, batch_size=32,\n",
    "#        epochs=config.num_epochs, verbose=1, callbacks=[\n",
    "#            WandbCallback(data_type=\"image\", labels=[\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"])\n",
    "#        ], validation_data=(val_faces, val_emotions))\n",
    "\n",
    "\n",
    "train_generator = datagen.flow(train_faces, train_emotions, batch_size=32)\n",
    "val_generator = datagen.flow(val_faces, val_emotions, batch_size=32)\n",
    "model.fit_generator(train_generator,\n",
    "            steps_per_epoch=len(train_faces)//32, \n",
    "            epochs=config.num_epochs, verbose=1,\n",
    "            callbacks=[\n",
    "                WandbCallback(data_type=\"image\", labels=[\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"], generator=val_generator)\n",
    "            ],\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=len(val_faces)//32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"emotion.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
